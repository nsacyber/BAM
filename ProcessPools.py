'''
This file contains the Manager Classes which handle the parts of Update process:
extracting from cab files, updating the database and searching for symbols
'''
#***************************************
# Imports
#***************************************
import asyncio
from codecs import backslashreplace_errors, xmlcharrefreplace_errors
from enum import Flag
from os.path import basename
import queue

import os
from signal import signal
import sqlite3

from time import time

import threading

import subprocess

import traceback as tb

import logging, logging.handlers

import hashlib

from concurrent.futures import ProcessPoolExecutor

from pathlib import Path

from typing import List, Tuple

import globs

from defusedxml import ElementTree as et

from db import wsuse_db

from support.utils import validatecab, ispe, validatezip, \
    getfilehashes, ispebuiltwithdebug, pebinarytype, getpepdbfilename, \
    getpeage, getpearch, getpesigwoage, ispedbgstripped, rmfile

import pefile

import MSDelta_imp


#****************************************************
# Local Variables
#****************************************************
_mgrlogger = logging.getLogger("BAM.Pools")

def mgr_logconfig(queue: queue.Queue):
    global _mgrlogger

    qh = logging.handlers.QueueHandler(queue)
    _mgrlogger.addHandler(qh)
    _mgrlogger.setLevel(logging.INFO)

def wkr_logconfig(queue: queue.Queue, logger: logging.Logger):
    parent = logger
    qh = logging.handlers.QueueHandler(queue)
    fh = logging.Formatter("[%(process)d][Thread %(thread)d] %(message)s")
    qh.setFormatter(fh)
    parent.addHandler(qh)
    parent.setLevel(logging.INFO)

#****************************************************
# Classes
#****************************************************
class CabMgr(threading.Thread):
    '''
    pdir - directory to cab files to be extracted
    dest - directory to place extracted results
    poolsize - number of processes to spawn and keep track of
    pemgr - PEMgr to pass results to
    db - DBMgr for which to refer database write jobs to
    '''
    def __init__(self, pdir: str, dest: str, poolsize: int, pemgr, psfxmgr, db, local: bool, globqueue: queue.Queue):
        super(CabMgr, self).__init__()
        self.pdir = pdir
        self.dest = dest
        self.poolsize = poolsize
        self.pemgr = pemgr
        self.psfxmgr = psfxmgr
        self.dbc = db
        # jobs - a list of cab files that have not been distributed to worker processes
        self.jobs = queue.Queue()
        # workRemaining - the amount of work that is not complete, which is not strictly the
        # same as the length of the jobs list, since work remaining includes jobs taken off
        # of the list to be worked on by processes
        self.workremaining = 0
        # jobsincoming - an event to signal to the ProcessPoolExecutor that more jobs have
        # been added to the jobs list
        self.jobsincoming = threading.Event()
        self.userinterrupt = threading.Event()
        self.localaction = local
        self.globqueue = globqueue
        self.cabmgrlogger = logging.getLogger("BAM.Pools.CabMgr")
         # self.daemon = True

    def addq(self, taskpath: str):
        '''
        adds item to job list from which the manager will task workers with
        '''
        self.jobs.put(taskpath)
        self.jobsincoming.set()
        self.workremaining += 1

    # TODO: since extract task should now handle internal cab files, we should
    # not deal with nestedcabs here so this part will need to be rewritten.
    def passresult(self, future: asyncio.Future):
        '''
        takes future generated by executor process and passes result to
        pemgr or psfxmgr
        '''
        # if exception generated by extracttask, notify here. If not, get result and send to
        # pemgr
        fexception = future.exception()
        if fexception is KeyboardInterrupt:
            self.cabmgrlogger.log(logging.INFO, "[CABMGR] execution stopped by user")
        elif fexception is not None:
            self.cabmgrlogger.log(logging.ERROR, "[CABMGR] {-} exception occurred: " + str(fexception) + \
                "\n\ttraceback: " + tb.format_exc())
        else:
            job = future.result()
            if job is not None:
                # if the directory contains ncabdir, then we
                # know that it is already a nestedCab, so remove
                # it from deliverables, otherwise, send to DBMgr
                if job[0] == "PSFX":
                    if job[1] is not None:
                        version = job[6]
                        for j in job[2]:
                            self.psfxmgr.addjob((j, version))
                    for j in job[2]:
                        self.psfxmgr.addjob(j)
                elif job[0] == "nonPSFX":
                    for j in job[1]:
                        self.pemgr.addjob(j)
                    self.cabmgrlogger.log(logging.INFO, "[CABMGR] sent to pemgr: " + str(job[0][0]))
                else:
                    self.cabmgrlogger.log(logging.INFO, "[CABMGR] no jobs passed to pemgr")
            else:
                self.cabmgrlogger.log(logging.INFO, "[CABMGR] job did not contain patched binaries or additional updates")

        # if there are no more nested cabs, set the jobs incoming event since there is at least 1 more job item to allow thread to
        # complete execution
        self.workremaining -= 1
        self.cabmgrlogger.log(logging.INFO, "[CABMGR] work remaining: " + str(self.workremaining))
        if self.workremaining == 0:
            self.jobsincoming.set()

    def queueDbTask(self, future: asyncio.Future):
        '''
        takes future generated by executor process and queues job for dbmanager, if applicable
        '''
        fexception = future.exception()
        if fexception is not None:
            self.cabmgrlogger.log(logging.ERROR, "[CABMGR] {-} exception occurred: " + str(fexception) + \
                "\n\ttraceback: " + tb.format_exc())
        else:
            job = future.result()
            if job is not None:
                jobtuple = (job[5], job[0])
                sha256 = job[3]
                sha1 = job[4]
                self.dbc.addtask("update", jobtuple, sha256, sha1, None)

    def run(self):
        '''
        CabMgr finds all cab files in target directory and adds to jobslist.
        Then assigns jobslist items to worker processes which handle extraction/DB update.
        Finally receives results back from workers, requeues nested cabs if any,
        and passes results to pemgr.
        '''
        start = time()
        self.cabmgrlogger.log(logging.INFO, "[CABMGR] starting CABMGR")
        if self.pemgr is not None:
            self.pemgr.cabstartsig()
        if self.psfxmgr is not None:
            self.psfxmgr.cabstartsig()
        # search for and add cab files to task queue
        for root, _, files, in os.walk(self.pdir):
            for file in files:
                filel = file.lower()
                if filel.endswith(".cab") or filel.endswith(".msu") \
                    or filel.endswith(".exe"):
                    self.addq(os.path.join(root, file))
        
        global _mgrlogger
        with ProcessPoolExecutor(max_workers=self.poolsize, initializer=wkr_logconfig, initargs=(self.globqueue, _mgrlogger)) as executor:
            # try:
            while not self.jobs.empty():
                if self.userinterrupt.is_set():
                    self.cabmgrlogger.log(logging.INFO, "[CABMGR] execution stopped by user")
                    break
                self.cabmgrlogger.log(logging.INFO, "[CABMGR] assigning extraction job")
                if self.localaction:
                    future = executor.submit(self.dbupdate, self.jobs.get(), \
                                                self.dest)
                else:
                    future = executor.submit(self.extracttask, self.jobs.get(), \
                                                self.dest)
                future.add_done_callback(self.passresult)
                future.add_done_callback(self.queueDbTask)
            # except KeyboardInterrupt:
            #     for pid in executor._processes:
            #         os.kill(pid, signal.SIGKILL)
            #     self.cabmgrlogger.log(logging.INFO, "[CABMGR] execution stopped by user")
                 
        
        if self.pemgr:
            self.pemgr.cabdonesig()
            self.cabmgrlogger.log(logging.INFO, "[CABMGR] done signal sent to pemgr")
        else:
            self.cabmgrlogger.log(logging.INFO, "[CABMGR] pemgr doesn't exist because working in update only mode")
        if self.psfxmgr:
            self.psfxmgr.cabdonesig()
            self.cabmgrlogger.log(logging.INFO, "[CABMGR] done signal sent to psfxmgr")
        else:
            self.cabmgrlogger.log(logging.INFO, "[CABMGR] psfxmgr doesn't exist because working in update only mode")

        self.dbc.donesig()

        end = time()
        elapsed = end-start
        print("**********************************part 1 done*************************************")
        print("elapsed time for part 1: " + str(elapsed))

    @staticmethod
    def verifyentry(src: str, sha256: str, sha1: str, logger: logging.Logger) -> bool:
        '''
        verify DB entry
        '''
        logmsg = "[CABMGR] Verifying entry for " + src
        logger.log(logging.INFO, logmsg)

        filepath = None

        try:
            filepath = str(Path(src).resolve())
        except OSError as error:
            logmsg = "[CABMGR] Issue getting full path for " + src + ": " + str(error) + ". Using unresolved path."
            logger.log(logging.ERROR, logmsg)
            filepath = src

        if wsuse_db.dbentryexist(globs.DBCONN.cursor(),     \
                                globs.UPDATEFILESDBNAME, sha256, sha1):
            logmsg = "[CABMGR] {-} item " + filepath + " already exists in db, skipping"
            logger.log(logging.WARNING, logmsg)
            return True
        return False

    @staticmethod
    def performcablisting(src: str, logger: logging.Logger) -> str|None:
        '''
        Call expand.exe to get a listing of files within a CAB/MSU
        '''
        result = None
        logmsg = "[CABMGR] Listing " + str(src) + " CAB contents"
        logger.log(logging.INFO, logmsg)

        filepath = os.environ['systemdrive'] + "\\Windows\\system32\\expand.exe"

        if not os.path.isfile(filepath):
            logmsg = "[CABMGR] {-} expand.exe was not found. Given path: " + filepath
            logger.log(logging.ERROR, logmsg)
            return None

        args = filepath + " -D \"" + str(src) + "\""

        try:
            with subprocess.Popen(args, shell=False, stdout=subprocess.PIPE) as pexp:
                intermediate, _ = pexp.communicate()
                result = intermediate.decode("ascii")
        except subprocess.CalledProcessError as error:
            logmsg = "[CABMGR] {-} Listing contents of " + src + \
                   " failed with " + str(error.returncode) + " " +  \
                   str(error.stderr)
            logger.log(logging.ERROR, logmsg)
            result = None
        except FileNotFoundError as error:
            logmsg = ("[CABMGR] {-} expand.exe not found")
            logger.log(logging.ERROR, logmsg)
            result = None

        return result

    @staticmethod
    def performcabextract(extstr: str, src: str, newdir: str, logger: logging.Logger) -> str|None:
        '''
        Call expand.exe to extract files (if any)
        https://support.microsoft.com/en-us/help/928636/you-cannot-extract-the-contents-of-a-microsoft-update-standalone-packa
        https://blogs.msdn.microsoft.com/astebner/2008/03/11/knowledge-base-article-describing-how-to-extract-msu-files-and-automate-installing-them/
        '''
        result = None
        args = os.environ['systemdrive'] + "\\Windows\\system32\\expand -R \"" + str(src) + "\" -F:" + str(extstr) + " \"" + str(newdir) + "\""
        try:
            with subprocess.Popen(args, shell=False, stdout=subprocess.PIPE) as pexp:
                rawstdout, _ = pexp.communicate()
                result = rawstdout.decode("ascii")
            logmsg = "[CABMGR] extracted " + extstr + " at " + newdir
            logger.log(logging.INFO, logmsg)
        except subprocess.CalledProcessError as error:
            logmsg = "[CABMGR] {-} extracting " + extstr + " from " + src +    \
                    " failed with " + str(error.returncode) + " " +  \
                    error.output.decode('ascii') + ".\n\n" + \
                    "{-} cmd (" + str(error.cmd) + ") stderr (" + \
                    str(error.stderr) + ")"
            logger.log(logging.ERROR, logmsg)
            result = None
        except FileNotFoundError as error:
            logmsg = ("[CABMGR] {-} expand.exe not found")
            logger.log(logging.ERROR, logmsg)
            result = None

        return result

    @staticmethod
    def perform7zextract(src: str, newpath: str, logger: logging.Logger):
        '''
        Call 7z.exe to extract files (if any)
        '''
        # stdout will be the std output of the 7zip subprocess
        stdout = None
        logmsg = "[CABMGR] Performing 7z on " + newpath
        logger.log(logging.INFO, logmsg)
        filepath = os.environ["PROGRAMW6432"] + "\\7-Zip\\7z.exe"
        
        if not os.path.isfile(filepath):
            logmsg = "[CABMGR] {-} 7z.exe was not found. Given path: " + filepath
            logger.log(logging.ERROR, logmsg)
            return None

        args =  filepath + " x -aoa -o\"" + str(newpath) + "\" -y -r \"" +str(src) + "\" *.dll *.sys *.exe"

        try:
            with subprocess.Popen(args, shell=False, stdout=subprocess.PIPE) as p7z:
                stdout, _ = p7z.communicate()
        except subprocess.CalledProcessError as error:
            logmsg = "[CABMGR] {-} extracting, using 7z, from " + src + \
                     " failed with " + str(error.returncode) + " " +  \
                     error.output.decode('ascii')
            logger.log(logging.ERROR, logmsg)
            stdout = None
        except FileNotFoundError as error:
            logmsg = ("[PBSK] {-} 7z.exe not found")
            logger.log(logging.ERROR, logmsg)
            stdout = None
        
        return stdout

    @classmethod
    def dbupdate(cls, src: str, pdestdir: str):
        '''
        when given a directory of updates (CABs/MSUs/ZIPs)
        and no extraction, only set up update files to be added to dbc.
        Destination is where patched files are.
        '''
        cablogger = logging.getLogger("BAM.Pools.cabwkr")
        
        logmsg = "[CABMGR][DBUP] starting on " + str(src)
        cablogger.log(logging.INFO, logmsg)

        # initialize deliverables
        deliverables = None
        newpath = ''

        # indicates that this cab is one of the new update version that MS started using for v1809 and forward
        # can't handle this type of update yet, so skip it.
        if "PSFX" in src or "psfx" in src:
            return deliverables

        hashes = getfilehashes(src)

        if hashes is None:
            return hashes

        if not (validatecab(src) or ispe(src) or validatezip(src)):
            logmsg = "[CABMGR][DBUP] invalid cab/pe/zip"
            cablogger.log(logging.ERROR, logmsg)
            return deliverables

        newname = src.split("\\")[-1].lstrip()
        newpath = pdestdir + "\\" + newname

        if ".exe" in newname:
            newpath = newpath.split(".exe")[0]
        elif ".cab" in newname:
            newpath = newpath.split(".cab")[0]
        elif ".zip" in newname:
            newpath = newpath.split(".zip")[0]

        deliverables = ((newpath, []), hashes[0], hashes[1])
        # No need to locate nested CABs/MSUs as long the parent update file
        # is found. Revisit if needed

        logmsg = "[CABMGR][DBUP] Extraction (DB update only) task completed for " + src
        cablogger.log(logging.INFO, logmsg)

        # Send the job to the next manager (DB will be updated eventually)
        return deliverables

    @classmethod
    def extracttask(cls, src: str, dst: str) -> Tuple:
        '''task for workers to extract contents of update files and return directory of
        results to PEMgr. Also gathers metadata and hands that information off to DBMgr
        to update the database with. When encountering a PSFX style update, will instead pass the 
        update to PSFXMgr for processing, and will allow PSFXMgr to update the db rather
        than itself.'''
        cablogger = logging.getLogger("BAM.Pools.cabwkr")

        # first check if information already exists in db
        hashes = getfilehashes(src)
        if hashes is None:
            raise Exception("Unable to calculate Hashes for update.")
        if cls.verifyentry(src, hashes[0], hashes[1], cablogger):
            return (None, None)

        # now check if src is an executable which will be extracted using 7zip
        if ispe(src):
            logmsg = "[CABMGR] extracting PE file (" + src + ")..."
            cablogger.log(logging.INFO, logmsg)
            newname = src.split("\\")[-1].lstrip()
            newdir = (dst + "\\" + newname).split(".exe")[0]
            try:
                os.mkdir(newdir)
            except FileExistsError:
                pass
            except OSError as oserror:
                logmsg = "[CABMGR] OSError creating new directory... skipping extraction for (" + \
                    src + "). Error: " + str(oserror)
                cablogger.log(logging.ERROR, logmsg)
                return None

            if cls.perform7zextract(src, newdir, cablogger) is None:
                # if nothing was extracted, remove the directory to clean up
                try:
                    os.rmdir(newdir)
                except OSError:
                    pass
                return None

            return ((str(newdir), []), hashes[0], hashes[1])
        # assuming that Microsoft is not devious enough to put a cab in a exe
        # proceed to extract cab/msu contents
        cabfile = src
        extractdir = dst + "\\" + src.split("\\")[-1][0:-4]
        try:
            os.mkdir(extractdir)
        except FileExistsError:
            pass

        # perform breadth first extraction
        cabqueue = queue.LifoQueue()
        cabqueue.put(cabfile) # preload queue so loop will work
        psfxlist = []
        nonpsfxlist = []
        packageFormat = "undetermined"
        version = 0

        while not cabqueue.empty():
            cabfile = cabqueue.get()
            if not cabfile == src:
                extractdir = "\\".join(cabfile.split("\\")[0:-1])

            listing = cls.performcablisting(cabfile, cablogger)
            
            # found that mum files in psfx cabs will have a psfx package format
            # attribute in the xml.
            if "update.mum" in listing:
                cls.performcabextract("update.mum", cabfile, extractdir, cablogger)
                mum = extractdir + "\\update.mum"
                version = cls.getVersion(mum)
                packageFormat = cls.getPackageFormat(mum)
            
            if cabfile != src:
                if version >= 17763 and packageFormat == "PSFX":
                    psfxlist.append(cabfile)
                else:
                    nonpsfxlist.append(cabfile)

            lines = listing.split("\r\n")
            for line in lines:
                # if a line contains a PE file, then we've reached the end of a branch,
                # so break and go to the next cab.
                if ".dll" in line or ".exe" in line or ".sys" in line:
                    break
                
                itemincab = line.split(" ")[-1].rstrip("\r\n\t ")
                if (itemincab.endswith(".cab") or itemincab.endswith(".msu")) and "WSUSSCAN" not in itemincab:
                    cabdir = extractdir + "\\" + itemincab[0:-4]
                    nextcab = cabdir + "\\" + itemincab
                    
                    cabqueue.put(nextcab)
                    try:
                        os.mkdir(cabdir)
                    except FileExistsError:
                        pass

                    cls.performcabextract(itemincab, cabfile, cabdir, cablogger)
            
        # for non psfx items, extract those contents immediately
        for cab in nonpsfxlist:
            cabdst = "\\".join(cab.split("\\")[0:-1])
            try:
                cls.performcabextract("*.dll", cab, cabdst, cablogger)
                cls.performcabextract("*.exe", cab, cabdst, cablogger)
                cls.performcabextract("*.sys", cab, cabdst, cablogger)

                os.remove(cab)
            except FileNotFoundError:
                logmsg = "cab file not found"
                cablogger.log(logging.ERROR, logmsg)

        DBupdateName = src.split("\\")[-1]
        if len(psfxlist) < 1:
            return ("nonPSFX", nonpsfxlist, None, hashes[0], hashes[1], DBupdateName, version)
        else:
            return ("PSFX", nonpsfxlist, psfxlist, hashes[0], hashes[1], DBupdateName, version)

    @classmethod
    def getPackageFormat(cls, xmlfile: str) -> str:
        '''read an xml file to find the packageformat of an update'''
        xml = et.parse(xmlfile)
        root = xml.getroot()
        for tag in root.iter('{urn:schemas-microsoft-com:asm.v3}customInformation'):
            if 'PackageFormat' in tag.attrib:
                # this will return the top level instance of version
                return tag.attrib['PackageFormat']
        return None

    @classmethod
    def getVersion(cls, xmlfile: str) -> int:
        '''read an xml file to find the version of an update'''
        xml = et.parse(xmlfile)
        root = xml.getroot()
        version = None
        for tag in root.iter('{urn:schemas-microsoft-com:asm.v3}assemblyIdentity'):
            if 'version' in tag.attrib:
                # this will return the top level instance of version
                version = tag.attrib['version']
                break
        if not version:
            raise Exception("Unable to find Version information")
        versionarr = version.split(".")
        # assuming we don't go past Windows 10 for now.
        if int(versionarr[0]) > 10:
            return int(versionarr[0])
        else:
            # sometimes we get the OS version rather than the Assembly version
            # so if we get the Assembly version, the buildnum is the first part
            # of the version number. If we get the OS version, the build num is
            # the third part.
            return int(versionarr[2])


class PSFXMgr(CabMgr):
    '''
    separate manager to handle psfx cabs files that cab manager doesn't have the tools to do.
    jobs will be queue of cab files
    '''
    def __init__(self, pdir: str, dest: str, poolsize: int, pemgr, db, local: bool, globqueue: queue.Queue, basedir: str):
        super(PSFXMgr, self).__init__(pdir, dest, poolsize, pemgr, self, db, local, globqueue)
        self.jobs = queue.Queue()
        self.cabmgrRunning = threading.Event()
        self.userinterrupt = threading.Event()
        # assumes that all base files needed are simply dumped into a single directory.
        self.basedir = basedir
        self.psfxmgrlogger = logging.getLogger("BAM.Pools.PSFXMgr")
         # self.daemon = True
    
    def run(self):
        start = time()
        self.cabmgrRunning.wait()
        self.pemgr.psfxstartsig()
        
        global _mgrlogger
        with ProcessPoolExecutor(max_workers=self.poolsize, initializer=wkr_logconfig, initargs=(self.globqueue, _mgrlogger)) as executor:
            # try:
            while self.cabmgrRunning.is_set() or not self.jobs.empty():
                if self.userinterrupt.is_set():
                    self.psfxmgrlogger.log(logging.INFO, "[PSFXMGR] execution stopped by user")
                    break
                try:
                    tuple = self.jobs.get(block=True, timeout=60)
                except queue.Empty:
                    # this is done because we know that we'll get instances of queue being empty
                    # but since we may still want to wait for other items to come in while the 
                    # previous manager is running, we need to ignore the queue being empty.
                    # Previously we used the regular get which blocked for waiting for another 
                    # item, but this ended up causing the thread to hang because the loop could
                    # not check the event that said the previous manager had completed.
                    continue
                psfxcab = tuple[0]
                version = tuple[1]
                dest = "\\".join(psfxcab.split("\\")[0:-1])
                future = executor.submit(self.PSFXExtract, psfxcab, dest, self.basedir, version)
                future.add_done_callback(self.passresult)
            # except KeyboardInterrupt:
            #     for pid in executor._processes:
            #         os.kill(pid, signal.SIGKILL)
            #     self.psfxmgrlogger.log(logging.INFO, "[PSFXMGR] execution stopped by user")
                 
        
        self.pemgr.psfxdonesig()
        self.psfxmgrlogger.log(logging.INFO, "[PSFXMGR] done signal sent to pemgr")
        end = time()
        elapsed = end - start
        print("*********************PSFX extraction done**********************")
        print("[PSFXMGR] time taken for psfx extract: ", elapsed)
        
    @classmethod
    def findBaseFile(cls, basepath: str, filepath: str, version: int, file: str) -> str:
        # all paths given should be full paths
        # basepath: the directory where every version of base file can be found
        # filepath: the directory of the delta files. Should point to 
        # the folder that contains both the forward and reverse dirs or the null dir
        # which would include the more descriptive name of the file.
        # version: the Windows build version.
        filedir = filepath.split("\\")[-1]

        updatedir = basepath + "\\" + str(version)
        if not os.path.exists(updatedir):
            raise FileNotFoundError("Base version ", str(version), " not found\n")

        if filedir in os.listdir(updatedir):
            for root, _, files in os.walk(os.path.join(updatedir, filedir)):
                if file in files:
                    return os.path.join(root, file)
        raise FileNotFoundError("unable to find base files " + filedir)

    @classmethod
    def PSFXExtract(cls, src: str, dest: str, basedir: str, version: int) -> str:
        # procesing code
        psfxwkrlog = logging.getLogger("BAM.Pools.psfxwkr")

        cls.performcabextract("*.dll", src, dest, psfxwkrlog)
        cls.performcabextract("*.exe", src, dest, psfxwkrlog)
        cls.performcabextract("*.sys", src, dest, psfxwkrlog)
        
        for root, dirs, files in os.walk(dest):
            if "f" in dirs or "r" in dirs or "n" in dirs:
                filesearch = root
            for file in files:
                if "\\n\\" in root or root.endswith("\\n"):
                    # change dest to place that makes sense later
                    null = root + "\\" + file
                    output = filesearch + "\\" + file
                    status, error = MSDelta_imp.patch_binary(None, None, None, output, null)
                    if not status == 0:
                        # error handling code here
                        logmsg = "Patching failed on " + file + " with Windows Error Code: " + error
                        psfxwkrlog.log(logging.INFO, logmsg)
                elif "\\r\\" in root or root.endswith("\\r"):
                    reverse = root + "\\" + file
                    forward = reverse.replace("\\r\\", "\\f\\")
                    output = filesearch + "\\" + file
                    try:
                        base = cls.findBaseFile(basedir, filesearch, version, file)
                    except FileNotFoundError:
                        logmsg = "Patching failed on " + file + ", base file not Found"
                        psfxwkrlog.log(logging.INFO, logmsg)
                        continue
                    status, error = MSDelta_imp.patch_binary(base, forward, reverse, output)
                    if not status == 0:
                        # error handling code here
                        logmsg = "Patching failed on " + file + " with Windows Error Code: " + error
                        psfxwkrlog.log(logging.INFO, logmsg)
                elif "\\f\\" in root or root.endswith("\\f"):
                    break

        # dest contains all the extracted content so just return that
        return dest

    def cabdonesig(self):
        self.cabmgrRunning.clear()

    def cabstartsig(self):
        self.cabmgrRunning.set()

    def addjob(self, psfxcab: str):
        # just add the cab to the queue.
        self.jobs.put(psfxcab)

    def passresult(self, future: asyncio.Future):
        fexception = future.exception()
        if fexception is KeyboardInterrupt:
            self.psfxmgrlogger.log(logging.INFO, "[PSFXMGR] execution stopped by user")
        elif fexception is not None:
            self.psfxmgrlogger.log(logging.ERROR, "[PSFXMGR] {-} exception occurred: " + str(fexception) + \
                "\n\ttraceback: " + tb.format_exc())
        else:
            pedir = future.result()
            self.pemgr.addjob(pedir)


class PEMgr(threading.Thread):
    '''
    after extraction is complete sorts through collected files and removes those that
    are already in the database. Finally passes remaining files to SymMgr for Symbol
    collection.
    poolsize - number of worker processes to spawn
    symmgr - SymbolMgr to pass results to
    db - DBMgr for which to refer database write jobs to
    '''
    def __init__(self, poolsize: int, symmgr, db, globqueue: queue.Queue):
        super(PEMgr, self).__init__()
        self.poolsize = poolsize
        self.symmgr = symmgr
        self.dbc = db
        self.globqueue = globqueue
        # jobs - holds the paths to the directories containing binaries to sort and
        # clean up
        self.jobs = queue.Queue()
        #  is an event to indicate that CabMgr has job items ready
        self.cabmgrRunning = threading.Event()
        self.psfxmgrRunning = threading.Event()
        self.userinterrupt = threading.Event()
        self.pemgrlogger = logging.getLogger("BAM.Pools.PEMgr")
         # self.daemon = True

    def addjob(self, pejobs: str):
        '''
        receive jobs from CabMgr, add those jobs to jobs List, and set
        the  Event to indicate there are jobs waiting to be
        processed
        '''
        self.jobs.put(pejobs)

    def cabstartsig(self):
        self.cabmgrRunning.set()
    
    def cabdonesig(self):
        '''
        Used to notify that there are no more jobs coming in from CabMgr
        so the last batch of work should proceed
        '''
        self.cabmgrRunning.clear()
        
    def psfxstartsig(self):
        self.psfxmgrRunning.set()

    def psfxdonesig(self):
        self.psfxmgrRunning.clear()

    def passresult(self, future: asyncio.Future):
        '''
        takes a future generated by the executor and passes result over to
        SymbolMgr for processing. Since result is guaranteed to be new to DB
        at this point, also submits result to DBMgr to update DB
        '''
        fexception = future.exception()
        if fexception is KeyboardInterrupt:
            self.pemgrlogger.log(logging.INFO, "[PEMGR] execution stopped by user")
        elif fexception is not None:
            self.pemgrlogger.log(logging.ERROR, "[PEMGR] {-} exception occurred: " + str(fexception) + \
            "\n\ttraceback: " + tb.format_exc())
        else:
            result = future.result()
            if result is not None:
                if result[3]["builtwithdbginfo"] and self.symmgr is not None:
                    # only need to use verify if PE was builtwithdbginfo here because 
                    # that condition takes precedence over the stripped condition and 
                    # if the item is stripped, a check must be made by symchk anyway 
                    # to find the .dbg file.
                    # Reference:
                    # https://docs.microsoft.com/en-us/windows-hardware/drivers
                    # /debugger/symchk-command-line-options
                    # in the DBG file options.
                    jobitem = (str(result[0][0]), result[1], result[2])
                    self.symmgr.addjob(jobitem)
                    self.pemgrlogger.log(logging.INFO, "[PEMGR] items passed to symmgr: " + str(result[0][0]))

                self.dbc.addtask("binary", result[0], result[1], result[2], result[3])

    def run(self):
        '''
        spawns, manages, and tasks workers to perform pe functionalities
        '''
        self.cabmgrRunning.wait()
        self.psfxmgrRunning.wait()
        self.pemgrlogger.log(logging.INFO, "[PEMGR] PEMgr starting")
        start_time = time()
        if self.symmgr is not None:
            self.symmgr.pestartsig()

        # setup workers and Executor
        global _mgrlogger
        with ProcessPoolExecutor(max_workers=self.poolsize, initializer=wkr_logconfig, initargs=(self.globqueue, _mgrlogger)) as executor:
            # try:
            while not self.jobs.empty() or self.psfxmgrRunning.is_set() or self.cabmgrRunning.is_set():
                if self.userinterrupt.is_set():
                    self.pemgrlogger.log(logging.INFO, "[PEMGR] execution stopped by user")
                    break
                try:
                    jobdir = self.jobs.get(block=True, timeout=60)
                except queue.Empty:
                    # this is done because we know that we'll get instances of queue being empty
                    # but since we may still want to wait for other items to come in while the 
                    # previous manager is running, we need to ignore the queue being empty.
                    # Previously we used the regular get which blocked for waiting for another 
                    # item, but this ended up causing the thread to hang because the loop could
                    # not check the event that said the previous manager had completed.
                    continue
                for root, _, files in os.walk(jobdir):
                    for file in files:
                        filepath = str(Path(os.path.join(root, file)).resolve())
                        self.pemgrlogger.log(logging.INFO, "[PEMGR] assigning pe job for " + str(filepath))
                        
                        # file naming convention in wsuscontent folder seems to be the same
                        # as whatever is at the end of update file obtained from update catalog
                        # so for our current purposes, searching via this regex is fine, but
                        # this format doesn't match with the UpdateID column in the database
                        # so correlation will have to be done in a different way?
                        # Using sha256 hash of file name for now, but need to figure something
                        # else out eventually since we may want to be able to correlate
                        # to the WSUS DB eventually. That would mean I have to find out how to
                        # get the real Update ID from somewhere.
                        bytes = str(filepath).encode('utf-8')
                        updateid = hashlib.sha256(bytes).hexdigest()

                        future = executor.submit(self.petask, str(filepath), updateid)
                        future.add_done_callback(self.passresult)
            # except KeyboardInterrupt:
            #     for pid in executor._processes:
            #         os.kill(pid, signal.SIGKILL)
            #     self.cabmgrlogger.log(logging.INFO, "[PEMGR] execution stopped by user")
                 
        
        self.pemgrlogger.log(logging.INFO, "[PEMGR] *************part 2 done*****************")
        print("[PEMGR] **************part 2 done**********************")

        if self.symmgr is not None:
            self.symmgr.pedonesig()
            self.pemgrlogger.log(logging.INFO, "[PEMGR] done signal sent to symmgr")
        else:
            self.pemgrlogger.log(logging.INFO, "[PEMGR] symmgr doesn't exist because working on update mode only")
        self.dbc.donesig()

        endtime = time()
        elapsedtime = endtime-start_time
        print("elapsed time for part 2: " + str(elapsedtime))

    @classmethod
    def petask(cls, jobfile: str, updateid: str) -> Tuple | None:
        '''
        task to process PEs before submitting jobs for symbol search
        if item is removed, None is returned, else return item
        '''
        clnlogger = logging.getLogger("BAM.Pools.ClnWkr")
        
        results = None

        logmsg = "[PEMGR] Starting on " + str(jobfile)
        clnlogger.log(logging.INFO, logmsg)

        if ispe(jobfile):
            # check db to see if job already exists:
            hashes = getfilehashes(jobfile)

            if hashes is None:
                return hashes

            if wsuse_db.dbentryexistwithsymbols(globs.DBCONN.cursor(),     \
                                globs.PATCHEDFILESDBNAME, hashes[0], hashes[1]):
                # if PE is already in db with symbols obtained,
                # do not retask job to symbol manager, return None instead
                return results
            else:
                pass
                logmsg = "[PEMGR] continuing forward with " + str(jobfile)
                clnlogger.log(logging.INFO, logmsg)

            # getting to this point means item is not in db, may need to come up
            # with case where db needs to update item though
            infolist = {
                    'OriginalFilename': '', 'FileDescription': '', 'ProductName': '',
                    'Comments': '', 'CompanyName': '', 'FileVersion': '',
                    'ProductVersion': '', 'IsDebug': '', 'IsPatched': '',
                    'IsPreReleased': '', 'IsPrivateBuild': '', 'IsSpecialBuild': '',
                    'Language': '', 'PrivateBuild': '', 'SpecialBuild': ''
                    }

            try:
                unpefile = pefile.PE(jobfile, fast_load=True)
            except pefile.PEFormatError as peerror:
                logmsg = "[WSUS_DB] skipping " + str(jobfile) + " due to exception: " + peerror.value
                clnlogger.log(logging.ERROR, logmsg)
                return results

            infolist['fileext'], infolist['stype'] = pebinarytype(unpefile)
            infolist['arch'] = getpearch(unpefile)
            infolist['age'] = getpeage(unpefile)
            infolist['strippedpe'] = ispedbgstripped(unpefile)
            infolist['builtwithdbginfo'] = ispebuiltwithdebug(unpefile)            

            direntires=[ pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_DEBUG'],    \
                pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_RESOURCE'] ]
            unpefile.parse_data_directories(directories=direntires)
            infolist['pdbfilename'] = getpepdbfilename(unpefile)
            infolist['signature'] = getpesigwoage(unpefile)

            # a PE only have 1 VERSIONINFO, but multiple language strings
            # More information on different properites can be found at
            # https://msdn.microsoft.com/en-us/library/windows/desktop/aa381058
            # https://msdn.microsoft.com/en-us/library/windows/desktop/aa381049
            if getattr(unpefile, "VS_VERSIONINFO", None) is not None and \
                getattr(unpefile, "FileInfo", None) is not None:
                    for fileinfoentries in unpefile.FileInfo:
                        for fileinfoentry in fileinfoentries:
                            if getattr(fileinfoentry, "StringTable", None) is not None:
                                for strtable in fileinfoentry.StringTable:
                                    # Currently only handling unicode en-us
                                    if strtable.LangID[:4] == b'0409' or \
                                            (strtable.LangID[:4] == b'0000' and
                                            (strtable.LangID[4:] == b'04b0' or
                                            strtable.LangID[4:] == b'04B0')):
                                        infolist["Language"] \
                                            = strtable.LangID.decode("utf-8")
                                        for field, value in strtable.entries.items():
                                            dfield = field.decode('utf-8')
                                            dvalue = value.decode('utf-8')
                                            if dfield == "OriginalFilename":
                                                infolist["OriginalFilename"] \
                                                    = dvalue
                                            if dfield == "FileDescription":
                                                infolist["FileDescription"] \
                                                    = dvalue
                                            if dfield == "ProductName":
                                                infolist["ProductName"] \
                                                    = dvalue
                                            if dfield == "Comments":
                                                infolist["Comments"] \
                                                    = dvalue
                                            if dfield == "CompanyName":
                                                infolist["CompanyName"] \
                                                    = dvalue
                                            if dfield == "FileVersion":
                                                infolist["FileVersion"] \
                                                    = dvalue
                                            if dfield == "ProductVersion":
                                                infolist["ProductVersion"] \
                                                    = dvalue
                                            if dfield == "IsDebug":
                                                infolist["IsDebug"] \
                                                    = dvalue
                                            if dfield == "IsPatched":
                                                infolist["IsPatched"] \
                                                    = dvalue
                                            if dfield == "IsPreReleased":
                                                infolist["IsPreReleased"] \
                                                    = dvalue
                                            if dfield == "IsPrivateBuild":
                                                infolist["IsPrivateBuild"] \
                                                    = dvalue
                                            if dfield == "IsSpecialBuild":
                                                infolist["IsSpecialBuild"] \
                                                    = dvalue
                                            if dfield == "PrivateBuild":
                                                infolist["PrivateBuild"] \
                                                    = dvalue
                                            if dfield == "SpecialBuild":
                                                infolist["SpecialBuild"] \
                                                    = dvalue
            # Get the OS this PE is designed towards.
            # Microsoft PE files distributed via Microsoft's Updates typically
            # use the ProductVersion file properties to indicate the OS a specific
            # PE file is built towards.
            # If this is a Microsoft binary, the Product version is typically
            # the OS version it was built towards, but other products this is not
            # necessarily true
            if infolist['ProductName'].find("Operating System") != -1:
                infolist['osver'] = "NT" + infolist['ProductVersion']
            else:
                infolist['osver'] = "UNKNOWN"

            unpefile.close()

            results = ((str(jobfile), updateid), hashes[0], hashes[1], infolist)
        else:
            # if jobfile is not a PE, then check if it's a cab. If not a cab, remove it.
            if not validatecab(str(jobfile)):
                logmsg = "[PEMGR] petask: Removing " + str(jobfile)
                clnlogger.log(logging.INFO, logmsg)

                rmfile(jobfile)

                logmsg = "[PEMGR] " + str(jobfile) + " removed, not PE or cab file"
                clnlogger.log(logging.INFO, logmsg)
            else:
                pass
                logmsg = "[PEMGR] " + str(jobfile) + " is nested cab, skipping"
                clnlogger.log(logging.INFO, logmsg)
            return results

        logmsg = "[PEMGR] completed one petask for " + str(jobfile)
        clnlogger.log(logging.INFO, logmsg)

        return results


class SymMgr(threading.Thread):
    '''
    poolsize - number of worker processes to spawn
    symserver - Symbol Server with which to refer to when using symchk
    symdest - destination folder for symbols if downloading new symbols
    dbc - DBMgr for which to refer database write jobs to
    '''
    def __init__(self, poolsize: int, symserver: str, symdest: str, db, symlocal:bool=False, globqueue:queue.Queue=None):
        super(SymMgr, self).__init__()
        self.poolsize = poolsize
        self.symserver = symserver
        self.symdest = symdest
        self.dbc = db
        self.symlocal = symlocal
        self.globqueue = globqueue
        # jobs - queue of files to search for symbols for
        self.jobs = queue.Queue()
        #  - an Event that indicates to SymMgr when jobs are ready
        # for it to process
        self.peRunning = threading.Event()
        self.userinterrupt = threading.Event()
        self.symmgrlogger = logging.getLogger("BAM.Pools.SymMgr")
         # self.daemon = True

    def pestartsig(self):
        '''
        indicates that PEMgr is running so continue to run even if queue is
        empty since more stuff could come down the pipeline
        '''
        self.peRunning.set()

    def pedonesig(self):
        '''
        Used to notify that there are no more jobs coming in from PEMgr
        so the last batch of work should proceed
        '''
        self.peRunning.clear()

    def addjob(self, jobset: Tuple):
        '''
        receive jobs from PEMgr, add those jobs to jobs List, and set
        the  Event to indicate there are jobs waiting to be
        processed
        '''
        if jobset is not None:
            self.jobs.put(jobset)

    def makedbrequest(self, future: asyncio.Future):
        '''
        once Symbols found, submit task to DBMgr to update database with found symbols
        '''
        fexception = future.exception()
        if fexception is KeyboardInterrupt:
            self.symmgrlogger.log(logging.INFO, "[SYMMGR] execution stopped by user")
        elif fexception is not None:
            self.symmgrlogger.log(logging.INFO, "[SYMMGR] {-} exception occurred: " + str(fexception) + "\ntraceback: " + \
                tb.format_exc())
        else:
            results = future.result()
            if results is not None:
                jobtuple = results[0]
                sha256 = results[1]
                sha1 = results[2]
                infolist = results[3]
                self.dbc.addtask("symbol", jobtuple, sha256, sha1, infolist)
            else:
                self.symmgrlogger.log(logging.INFO, "[SYMMGR] no symbols found")

    def run(self):
        '''
        spawns, manages, and tasks workers to perform pe functionalities
        '''
        self.peRunning.wait()
        start_time = time()
        # setup workers and Executor
        global _mgrlogger
        with ProcessPoolExecutor(max_workers=self.poolsize, initializer=wkr_logconfig, initargs=(self.globqueue, _mgrlogger)) as executor:
            # try:
                # take item from jobs and assign to workers
            while not self.jobs.empty() or self.peRunning.is_set():
                if self.userinterrupt.is_set():
                    self.symmgrlogger.log(logging.INFO, "[SYMMGR] execution stopped by user")
                    break
                try:
                    job = self.jobs.get(block=True, timeout=60)
                except queue.Empty:
                    # this is done because we know that we'll get instances of queue being empty
                    # but since we may still want to wait for other items to come in while the 
                    # previous manager is running, we need to ignore the queue being empty.
                    # Previously we used the regular get which blocked for waiting for another 
                    # item, but this ended up causing the thread to hang because the loop could
                    # not check the event that said the previous manager had completed.
                    continue
                self.symmgrlogger.log(logging.INFO, "[SYMMGR] assigning symbol job")
                future = executor.submit(self.symtask, job, self.symserver, \
                    self.symdest, self.symlocal)
                future.add_done_callback(self.makedbrequest)
            # except KeyboardInterrupt:
            #     for pid in executor._processes:
            #         os.kill(pid, signal.SIGKILL)
            #     self.cabmgrlogger.log(logging.INFO, "[SYMMGR] execution stopped by user")
                 
            print("[SYMMGR] job queue empty and previous manager complete, shutting down SYMMgr.")

        self.symmgrlogger.log(logging.INFO, "[SYMMGR] *************part 3 done*****************")
        print("[SYMMGR] **************part 3 done**********************")

        self.dbc.donesig()

        endtime = time()
        elapsedtime = endtime-start_time
        print("elapsed time for part 3: " + str(elapsedtime))

    @classmethod
    def symtask(cls, jobitem: Tuple, symserver: str, symdest: str, symlocal: bool) -> Tuple | None:
        '''
        perform symbol search for symbols of jobfile. If there are no symbols or symbols found
        are already in db, discard results. Else, return found symbols
        '''
        symlogger = logging.getLogger("BAM.Pools.SymWkr")
        jobfile = jobitem[0]
        hashes = (jobfile[1], jobfile[2])

        result = None
        logmsg = "[SYMMGR].. Getting SYM for (" + str(jobfile) + ")"
        symlogger.log(logging.INFO, logmsg)
        servers = ""

        if symlocal:
            servers = " \""+ symdest + "\""
        else:
            servers = "u \"srv*" + symdest + "*" + symserver +"\""

        args = (".\\tools\\x64\\symchk.exe /v \"" + str(jobfile) + "\" /s" + servers + " /od")

        try:
            with subprocess.Popen(args, shell=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE) \
                    as psymchk:
                # communicate used as symchk's output is for one file and
                # is not "large or unlimited"
                pstdout, pstderr = psymchk.communicate()
                stdoutsplit = str(pstdout.decode("ascii")).split("\r\n")
                stderrsplit = str(pstderr.decode("ascii")).split("\r\n")

                logmsg = "[SYMMGR] Attempt to obtain symbols for " + str(jobfile) + " complete"
                symlogger.log(logging.INFO, logmsg)

                infolist = {}
                try:
                    unpefile = pefile.PE(jobfile)
                except pefile.PEFormatError as peerror:
                    logmsg = "[WSUS_DB] Caught: PE error " + str(peerror) + ". File: " + jobfile
                    symlogger.log(logging.ERROR, logmsg)
                    return result

                infolist['signature'] = getpesigwoage(unpefile)
                infolist['arch'] = getpearch(unpefile)

                unpefile.close()

                stderrsplit.append(symserver)
                result = ((str(jobfile), stderrsplit, stdoutsplit), hashes[0], hashes[1], infolist)
        except subprocess.CalledProcessError as error:
            logmsg = "[SYMMGR] {-} symchk failed with error: " + str(error) + ". File: " + jobfile
            symlogger.log(logging.ERROR, logmsg)
            result = None
        except FileNotFoundError as error:
            logmsg = ("[SYMMGR] {-} symchk.exe not found")
            symlogger.log(logging.ERROR, logmsg)
            result = None

        logmsg = "[SYMMGR] completed symtask for " + str(jobfile)
        symlogger.log(logging.INFO, logmsg)
        return result


class DBMgr(threading.Thread):
    '''
    handles all write transactions to db. other managers needing to write to db will
    submit request to DBmgr which will accept and perform transaction to prevent race
    conditions.
    dbConn - sqlite3 connection to database
    '''
    def __init__(self, exdest: str, dbconn:sqlite3.Connection=None):
        super(DBMgr, self).__init__()
        self.dbconn = dbconn
        # jobqueue - queue to hold database write tasks
        self.jobqueue = queue.Queue()
        # jobsig - event that indicates when there are jobs that are waiting to be processed
        self.jobsig = threading.Event()
        self.userinterrupt = threading.Event()
        # donecount - used to indicate when there are no more jobs being submit to the DBMgr
        # by any of the other 3 Mgrs. When count is 3, indicates that there are no more jobs
        self.donecount = 0
        self.dbrecordscnt = 0
        self.exdest = exdest
        self.dblogger = logging.getLogger("BAM.Pools.DbMgr")
         # self.daemon = True

    def addtask(self, optype: str, jobtuple: Tuple, sha256: str, sha1: str, infolist: dict[str,str]):
        '''
        takes results generated from other Mgrs and creates a task which is placed
        in the Queue
        '''
        task = (optype, jobtuple, sha256, sha1, infolist)
        self.jobqueue.put(task)
        self.dblogger.log(logging.INFO, "[DBMGR] " + optype + " task added to queue. Queue at " + \
               str(self.jobqueue.qsize()) + " tasks.")
        self.jobsig.set()

    def writeupdate(self, file: str, sha256: str, sha1: str):
        '''
        performs writes to DB for Update files
        should use function in wsuse_db
        '''
        self.dblogger.log(logging.INFO, "[DBMGR] writing update for (" + str(file) + ")")
        wsuse_db.writeupdate(file, sha256, sha1, conn=self.dbconn)

    def writebinary(self, file: str, updateid: str, sha256: str, sha1: str, infolist: dict[str,str]):
        '''
        performs write updates to db for Binary files
        should use function in wsuse_db
        '''
        self.dblogger.log(logging.INFO, "[DBMGR] writing binary for (" + str(file) + ")")
        wsuse_db.writebinary(file, updateid, sha256, sha1, infolist, conn=self.dbconn)

    def writesym(self, file: str, symchkerr: str, symchkout: str, sha256: str, sha1: str, infolist: dict[str,str]):
        '''
        performs write updates to db for Symbol Files
        should use function in wsuse_db
        '''
        self.dblogger.log(logging.INFO, "[DBMGR] writing symbol for (" + str(file) + ")")

        wsuse_db.writesymbol(file, symchkerr, symchkout, sha256, sha1, infolist, self.exdest, conn=self.dbconn)

    def donesig(self):
        '''
        When all other Mgrs are done working, set jobsig so DBMgr can handle
        rest of items in queue and complete
        '''
        self.donecount += 1
        if self.donecount >= 3:
            # set the flag so that the rest of the queue can be emptied out.
            self.jobsig.set()

    def run(self):
        '''
        handles requests from other Mgrs to write to DB
        '''
        self.dblogger.log(logging.INFO, "[DBMGR] DBMgr starting")
        start_time = time()
        in_transaction = False

        # try:
        while self.donecount < 3:
            if self.userinterrupt.is_set():
                self.dblogger.log(logging.INFO, "[DBMGR] execution stopped by user.")
                # self.dbconn.interrupt()
                break
            if self.jobqueue.empty():
                self.dblogger.log(logging.INFO, "[DBMGR] waiting for more database jobs")
                self.jobsig.wait()

            # once signal received, take tasks off queue and process
            while not self.jobqueue.empty():

                # For every 5k queries, end transaction, commit, then restart
                # transaction
                self.dblogger.log(logging.INFO, "[DBMGR][DBUP] " + str(self.dbrecordscnt) + " records ready...")

                if self.dbrecordscnt == 0 and not in_transaction:
                    # this case is only run once
                    print("[DBUP] Restarting limit count")
                    wsuse_db.starttransaction(self.dbconn)
                    in_transaction = True
                elif self.dbrecordscnt >= 5000 and in_transaction:
                    print("[DBUP] limit to commit hit")
                    wsuse_db.endtransaction(self.dbconn)
                    self.dbrecordscnt = 0
                    in_transaction = False

                task = self.jobqueue.get()
                self.dblogger.log(logging.INFO, "[DBMGR] assigning database job: " + str(task[0]))

                if task[0] == "update":
                    self.dbrecordscnt += 1
                    self.writeupdate(task[1][0], task[2], task[3])
                elif task[0] == "binary":
                    self.dbrecordscnt += 1
                    self.writebinary(task[1][0], task[1][1], task[2], task[3], task[4])
                elif task[0] == "symbol":
                    self.dbrecordscnt += 1
                    self.writesym(task[1][0], task[1][1], task[1][2], task[2], task[3], task[4])
                else:
                    self.dblogger.log(logging.INFO, "[DBMGR] task unrecognized")

                self.dblogger.log(logging.INFO, "[DBMGR] task done, queue at " + str(self.jobqueue.qsize()) + " tasks")

                if self.jobsig.is_set():
                    self.jobsig.clear()
        # except KeyboardInterrupt:
        #     self.dblogger.log(logging.INFO, "[DBMGR] execution stopped by user.")

        if in_transaction:
            wsuse_db.endtransaction(self.dbconn, True)
            in_transaction = False
        endtime = time()
        elapsedtime = endtime-start_time
        print("elapsed time for part 4: " + str(elapsedtime))
        self.dblogger.log(logging.INFO, "[DBMGR] ****************everything done********************")
        print("[DMMGR] **************everything done**********************")
